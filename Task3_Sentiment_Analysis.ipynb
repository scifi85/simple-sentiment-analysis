{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_csv('Sentiment Analysis Dataset.csv', error_bad_lines=False, sep=',')\n",
    "df = raw_df.loc[:, ['Sentiment', 'SentimentText']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">SentimentText</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>788435</td>\n",
       "      <td>788435</td>\n",
       "      <td>@mcraddictal No you aren't honey  ILY &amp;lt;3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>790177</td>\n",
       "      <td>790177</td>\n",
       "      <td>@Uncle_Trav not following me?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SentimentText                                                       \\\n",
       "                  count  unique                                          top   \n",
       "Sentiment                                                                      \n",
       "0                788435  788435  @mcraddictal No you aren't honey  ILY &lt;3   \n",
       "1                790177  790177               @Uncle_Trav not following me?    \n",
       "\n",
       "                \n",
       "          freq  \n",
       "Sentiment       \n",
       "0            1  \n",
       "1            1  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Sentiment').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive = df[df['Sentiment']>0]\n",
    "df_negative = df[df['Sentiment']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>or i just worry too much?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ...\n",
       "5          0                  or i just worry too much?        "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_negative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(data, n):\n",
    "    # Use CountVectorizer to get Baf-Of-Words. Remove stop words and punctuation\n",
    "    count_vect = CountVectorizer(stop_words='english').fit(data)\n",
    "    bag_of_words = count_vect.transform(data)\n",
    "\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    # Sort words in corpus by their frequency\n",
    "    freq = [(word, sum_words[0, i]) for word, i in count_vect.vocabulary_.items()]\n",
    "    freq = sorted(freq, key = lambda x: x[1], reverse=True)\n",
    "    return freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('just', 62422),\n",
       " ('good', 60795),\n",
       " ('love', 47573),\n",
       " ('http', 46223),\n",
       " ('day', 45951),\n",
       " ('quot', 45471),\n",
       " ('like', 37451),\n",
       " ('lol', 36000),\n",
       " ('com', 34833),\n",
       " ('thanks', 34434)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOP 10 positive words\n",
    "get_top_n_words(df_positive['SentimentText'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('just', 63795),\n",
       " ('work', 44065),\n",
       " ('like', 40882),\n",
       " ('day', 39498),\n",
       " ('today', 37859),\n",
       " ('going', 33416),\n",
       " ('got', 33031),\n",
       " ('don', 32526),\n",
       " ('really', 31172),\n",
       " ('im', 30987)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOP 10 negative words\n",
    "get_top_n_words(df_negative['SentimentText'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('just', 63795),\n",
       " ('work', 44065),\n",
       " ('like', 40882),\n",
       " ('day', 39498),\n",
       " ('today', 37859),\n",
       " ('going', 33416),\n",
       " ('got', 33031),\n",
       " ('don', 32526),\n",
       " ('really', 31172),\n",
       " ('im', 30987)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOP 10 negative words\n",
    "# negative_freq[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipf law\n",
    "Zipf law basically says that if we sort all words in some language by their frequency from the most frequent to the least, we will see that frequency value will be reduced almost in the same proportions like word's rank.\n",
    "For example frequency value of the 3rd word will be 3 times smaller than frequency value of the 1st word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP words from the whole corpus\n",
    "count_vect = CountVectorizer(stop_words='english').fit(df['SentimentText'])\n",
    "bag_of_words = count_vect.transform(df['SentimentText'])\n",
    "sum_of_words = bag_of_words.sum(axis=0)\n",
    "word_freq = [sum_of_words[0, i] for _, i in count_vect.vocabulary_.items()]\n",
    "word_freq.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = list(range(1, 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ranks, word_freq[:30], 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that chart shows that Zipf law works not as good as we expected. 3rd word frequency is not 3 times smaller than 1st. If we don't remove stop words from the corpus results are a little bit better but not ideal.\n",
    "I think this belongs to the size of the dataset, words uniqueness and specific slang in the comments (when we have a lot of different word's spelling etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['SentimentText'].values\n",
    "y = df['Sentiment'].values\n",
    "texts_train, texts_test, y_train, y_test = train_test_split(texts, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "\n",
    "X_train_tokenized = tokenizer.texts_to_sequences(texts_train)\n",
    "X_test_tokenized = tokenizer.texts_to_sequences(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train_tokenized, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test_tokenized, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644089"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, LSTM, GlobalMaxPool1D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(maxlen, ))\n",
    "\n",
    "x = Embedding(max_features, 128)(inp)\n",
    "\n",
    "# use LSTM for our neural network as a nice common way to work with embeddings\n",
    "x = LSTM(60, return_sequences=True)(x)\n",
    "\n",
    "# reduce size\n",
    "x = GlobalMaxPool1D()(x)\n",
    "\n",
    "# helps to prevent overfitting\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# just workers :)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1278675 samples, validate on 142075 samples\n",
      "Epoch 1/1\n",
      "1278675/1278675 [==============================] - 4174s 3ms/step - loss: 0.4127 - acc: 0.8110 - val_loss: 0.4011 - val_acc: 0.8172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eff38546cc0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 1\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.9346451}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = pad_sequences(tokenizer.texts_to_sequences([\"I love you\"]), maxlen=maxlen)\n",
    "dict(zip([1, 0], model.predict(inp)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38556106567475107\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8251951434102546\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to impove this model:\n",
    "- Use bigger dataset. Size of the dataset is crucial for the NN\n",
    "- Use pretrained Embeddings like Word2Vec\n",
    "- Play with hyperparameters. Grid search helps here, but we should use much powerful computers and GPU to reduce training time.\n",
    "- Try different model's architecture (without LSTM, more Dense layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
